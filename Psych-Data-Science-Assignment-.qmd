---
title: "Psych Data Science Lab @ BGU 2025 -Group GLAS"
author: "Gilad Sarusi, Lior Lipa, Adi Bitan and Sivan Dayan"
output: 
  pagedown::html_paged:
      self_contained: true
      paged-footnotes: true
      
format: 
  html:
    toc: true
    number-sections: true

editor: visual
---

# The Objective

Can chains of spontaneous thought reveal mental health? In this competition, you'll work with rich psychological data to predict depression levels from free association patterns and demographic variables.

Using a dataset of participants who completed a depression questionnaire (CES-D) and a 10-round free association task, our goal is to predict each participant’s depression score using their unique trails of thought and background information.

For details about the challenge visit it's Kaggle page: [Depression Prediction Competitio](https://www.kaggle.com/competitions/depression-prediction/overview)

#Feature Engineering

{in this part we would add the acedemic background that helped us choose our feature and to desgin them in the way we will}

First and foremost, we will load th data.

```{r}
library(tidyverse)
library(tidymodels)
library(quanteda)
library(embedplyr)
library(pdftools)
library(stringr)
library(text)
training_set <- read.csv('./data/ds_train_full.csv')

# DO NOT TOUCH THIS ONE! 
test_set <- read.csv('./data/ds_test_full.csv')

# The prompt given to the subjects in each block:
prompts_given <- data.frame(
  prompt = c(
  "log",
  "orb",
  "extension",
  "synthesizer",
  "treadmill",
  "valet",
  "toss",
  "testimony",
  "mow",
  "forge"),
  order = 1:10
)




# Since each subject goes through 10 blocks that has 10 steps it means that our training set consists of 51800/100 subjects (518)

# Since that this is Goal oriented challange where the justification of using one method instead of the other is relevant, we would use two method to split our traning set for validationL bootstrap (enable us to asses confidence) and Kfold CV (the standard and straightforward approach)

# NOTICE, the rows are not independent of one another- each subject has 100 rows so when splitting we need to ensure there's no data linkage. 

#Another consideration is class balance, we'll need to think whether or not we want strata sampling in bootstraps, meaning is there's any meaningful classes in training sets that might tilt the results towards one way due to unbalanced sampling in validation set vs the remaining set 

# ------ Pre-processing

#School need to be re arrange by order 
levels(factor(training_set$school))


# Income need to be re arrange by order 
levels(factor(training_set$Income))


hist(training_set$Age)
education_levels <- c(
  "Less than high school degree",
  "High school graduate (high school diploma or equivalent including GED)",
  "Some college but no degree",
  "Associate degree in college (2-year)",
  "Bachelor's degree in college (4-year)",
  "Master's degree",
  "Professional degree (JD, MD)",
  "Doctoral degree"
)
income_range_levels <- c(
  "Less than $10,000",
  "$10,000 to $19,999",
  "$20,000 to $29,999",
  "$30,000 to $39,999",
  "$40,000 to $49,999",
  "$50,000 to $59,999",
  "$60,000 to $69,999",
  "$70,000 to $79,999",
  "$80,000 to $89,999",
  "$90,000 to $99,999",
  "$100,000 to $149,999",
  "$150,000 or more"
)

#-------------------- Corpus, Embedding and Dictionaries ----------------

# Since   "docnames must be unique" we would create a word_id for every unique word

training_corpus <- training_set |>
  mutate(word_id =row_number()) |>  
  corpus(docid_field = "word_id",
         text_field="word")


# In order to use word_count methods we need that each participant's words (all 100 words) assign to him would be regarded as a text (let's say treatign his chains of words as one big 'tweet') then we would be able to count the the negative words in his chain

training_corpus_per_subject <- training_set |>
  group_by(ID) |>
  summarise(words_all = paste(word, collapse = " "), .groups = "drop") |>
  ungroup() |>  
  corpus(docid_field = "ID",
         text_field="words_all") 



# Tokenazation 

#due to the nature of the task I don't see aby reason to stems or lemmas, the same goes for not using N-Gram - the task's output is one-word each step


#Tokenazing the words with the current data structure might be useful later 

training_tokens <- training_corpus |>
  tokens(  
    remove_punct = T,
    remove_symbols = T,
    remove_numbers = T) |>
  tokens_tolower()


neg_sentiment_words <-read.table("data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt",
                                 header = TRUE, sep = "\t") |>
  mutate(word = aback,
         sentiment = factor(anger),
         associate= X0) |>
  select(-aback,-anger,-X0) |>
  filter(sentiment== "negative" & associate == "1")


#------------------ Feature I - Negative Word Count -----------------  

training_corpus_per_subject_token_dfm <- training_corpus_per_subject  |>
  tokens(  
    remove_punct = T,
    remove_symbols = T,
    remove_numbers = T) |>
  tokens_tolower() |>
  dfm()


training_perS_neg_count <-training_corpus_per_subject_token_dfm |>
  dfm_lookup(
    dictionary(
      list(
        negative_count = neg_sentiment_words$word)
    )
  ) 

# let's add it to our training set

training_set <- training_set |>
  left_join(
    training_perS_neg_count |>
      convert("data.frame") |>
      rename(ID = doc_id) |>
      mutate(ID = as.numeric(ID)),
    by = "ID")

#Sanity Check:

table(training_set$negative_count)

hist(training_set$negative_count)

#VERY NICE (BORAT VOICING)!!! 


# -------------- Feature II - Embedded CCR (CES-D) -----------------

# The heavy lifting starts now! Let's enter the multidimentional vector world 

# First I would fetch CES-D questioneare using 'stringr' and 'pdftools' packages:

CES_D_items <- pdf_text("./data/epidemiologic-studies-scale.pdf") |>
  str_split("\\n(?=[0-9]+\\.)")  |>
  unlist() |>
  # keep only lines that start with number + dot
  str_subset("^[0-9]+\\.") |>
  # clean whitespace
  str_trim() |>
  # remove the leading number + dot + space, keep only the text
  str_remove("^[0-9]+\\.\\s*") |>
  tibble::tibble(item_number = 1:20,     
                 item_text   = _) |>
  
  # Dealing with reversed items 
  
  mutate(item_text = case_when(
    item_number == 4 ~ "I felt I was not as good as other people.",
    item_number == 8 ~ "I didn't felt hopeful about the future.",
    item_number == 12 ~ "I wasn't happy.",
    item_number == 16 ~ "I didn't enjoyed life.",
    
    #. Fixing Bad Parsing 
    item_number == 20 ~ "I could not get “going.",
    TRUE ~ item_text
  )) |>
  select(item_text) |>
  pull()

sbert_embeddings <- function(texts) {
  textEmbed(
    texts,
    model = "sentence-transformers/all-MiniLM-L12-v2", 
    layers = -2,  # second to last layer (default)
    tokens_select = "[CLS]", # use only [CLS] token
    dim_name = FALSE,
    keep_token_embeddings = FALSE
  )$texts[[1]]
} 


#It makes more sense to pass the text of single words and not the combine per subject since
# the contex are in effect when it comes to the word embedding of that word- since this is a free association task it should be forged into a non-sensed sentence.

training_set <- training_set |>
  
  # every unique word gets an ID 
  mutate(wordId = dense_rank(word))



# training_corpus_sbert <- training_set |>
#   embed_docs(
#     text_col = "word", 
#     model = sbert_embeddings, 
#     id_col = "wordId", 
#     .keep_all = TRUE
#     )
#Save it as RDS to reduce computation time upon relaunch
#     saveRDS(training_corpus_sbert,
#        file = "./rds/training_corpus_sbert.rds")
training_corpus_sbert <-readRDS("./rds/training_corpus_sbert.rds")


depression_ccr <- CES_D_items |> 
  embed_docs(sbert_embeddings, output_embeddings = TRUE) |> 
  average_embedding()


    
#getting the Cos Similiarity of each word to the CCR :

WordId_depression_ccr <- training_corpus_sbert |>
  get_sims(
    Dim1:Dim384,
    list(depression = depression_ccr),
    method = "cosine_squished"
    )



#Aggregating it to subject level not word level 


subject_cos_sim <- WordId_depression_ccr |>
  group_by(ID) |>
  summarise(
    dep_cos_sim = mean(depression, na.rm = TRUE),
    dep_sim_sd  = sd(depression, na.rm = TRUE),
  ) |>
  ungroup()
hist(subject_cos_sim$dep_cos_sim)


# Adding it to our Training set :

training_set <- training_set |> 
  left_join(subject_cos_sim,
            by ="ID")


#---------------------- Feature III - DDR  -----------------------



# Unlike CCR, where the items are sentences and the context of words affects their meaning, DDR follows a dictionary-based approach. Therefore, we would use word2vec for DDR embeddings, more suiting for static word embedding .

#How many association are word and how many are sentences ?

n_multi <- sum(str_detect(training_set$word, "\\S\\s+\\S"), na.rm = TRUE)
prop_multi <- mean(str_detect(training_set$word, "\\S\\s+\\S"), na.rm = TRUE)

# less than 2% 



#Since Word2Vec already uses static meaning of the words, use the per_subject df (in each all the 100 words are stacked in one string from each subject) for our embedding. Also, this aligns with the idea that every participant would get one DDR score 

# options(download.file.method = "libcurl", timeout = 1200)
# Sys.setenv(CURL_HTTP_VERSION = "1.1")
#            
# word2vec_model <- load_embeddings(
#   model  = "cc.en.300", 
#   words  = featnames(training_corpus_per_subject_token_dfm),
#   format = "rds",                          
#   dir    = "./rds"
# )    

word2vec_model<-readRDS("./rds/cc.en.300.rds")



#Traing the model on own corpus 
word2vec_training_set <- training_corpus_per_subject_token_dfm |>
  textstat_embedding(word2vec_model)



# Creating the DDR :

negative_word2vec_ddr <- predict(word2vec_model,
                               neg_sentiment_words$word) |> 
      average_embedding(w= "trillion_word", na.rm = T)

# Warning: 2401 items in `newdata` are not present in the embeddings object.
# the DDR is of 916 negative words 


neg_mood_ddr_cossim <- word2vec_training_set |> 
  get_sims(
    dim_1:dim_300, 
    list(negative_DDR = negative_word2vec_ddr), 
    method = "cosine_squished"
    ) |>
  rename(ID= doc_id) |>
  mutate(ID = as.numeric(ID))



#let's add that feature into our training set 

training_set <- training_set |>
  left_join(neg_mood_ddr_cossim,
            by= "ID")




# ------------ Feature IV - Semantic Distance (first Word)--------------

# Several studies suggest that depression—especially rumination—is characterized by fixation on a narrow, predominantly negative semantic space. Consequently, I expect smaller semantic distances among associations in depressed participants.


#Let's get the embedding of each word in the corpus, aka all the words subject use in the task. Firstly, we would embedd the prompted words: 

E <- as.matrix(predict(word2vec_model, prompts_given$prompt))
prompts_embedding <- data.frame(
  prompt_n = 1:nrow(E),
  word = rownames(E),
  E
)
for (q in 1:10) {
  # We are running on the first word given in each chain (cycle - where q_num == 1)
  # getting the prediction of the embedding by the model, adding the embedding to
  # the q1_words df. The q lop is needed because we compare the cos_sim to DIFFENERT   # word each cycle 
  
  q1_words <- training_set|>
    filter(q_num ==1) |>
    mutate(word = tolower(word))|> 
    select(word,
           wordId,
           cycle,
           q_num,
           ID)
  
  I1_emd <- as.matrix(predict(word2vec_model, unique(q1_words$word))) |>
    as.data.frame() |>
    rownames_to_column("word")
  
  # Adding the embedding to first words 
  q1_words <- q1_words |>
    left_join(I1_emd, by = "word") |>
    mutate(w2v_oov = if_else(!word %in% I1_emd$word, TRUE, FALSE))
  
  #Warning: 54 items in `newdata` are not present in the embedding object.
  
  
  d_prompt1_to_word1 <- q1_words  |> 
    get_sims(
      dim_1:dim_300, 
      list(d_0_1 = prompts_embedding |> 
             filter(prompt_n == q) |> 
             select(-word,-prompt_n) |>
             as.numeric()),
      method = "cosine_squished"
    )
}

# wide for each subject and add it to training set 
training_set <- training_set |>
  left_join(d_prompt1_to_word1 |>
              pivot_wider(
                id_cols = ID,
                names_from = cycle,
                names_glue = "d_0_1_c{cycle}",
                values_from = d_0_1,
                values_fill = NA_real_
              ),
            by ="ID") |>
  
  # creating a mean Distance from prompt to word 1  
  mutate(
    d_0_1_mean = rowMeans(pick(starts_with("d_0_1_c")), na.rm = TRUE)
  )































#--------------- Recipe ---------------------



rec <- recipe(matrixScore ~ ., data = training_set) |>
  step_rename(
    school_text = school,
    Income_text = Income
  ) |>
  step_mutate(
    
    
    #Setting the leveled vector  for school 
    school_text = factor(school_text,
                         levels = education_levels,
                         ordered = TRUE),
    # Creating an ordered school numeric column
    school_level = as.integer(school_text),
    
    #--------do the same for income: -----------
    Income_text = factor(Income_text,
                         levels=income_range_levels,
                         ordered = TRUE),
    Income_level = as.integer(Income_text),
    
    #------- Factoring the needed columns -------
    race_1 = factor(race_1),
    race_2 = factor(race_2),
    race_3 = factor(race_3),
    race_4 = factor(race_4),
    race_5 = factor(race_5),
    # since the 'Other' option in race_6 is manually typed I would regard it all a other even though we might grouped it in other manner
    race_6 = factor(race_6),
    
    sex = factor(sex)
    


  ) |>
  step_rm(
    Income_text,
    Income_level
  ) 

  

# See if the steps done correctly 
view(bake(prep(rec, training_set = training_data), NULL))




# ------ Validation ----------




```

# Modeling 

## Validation

