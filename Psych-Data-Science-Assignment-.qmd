---
title: "Psych Data Science Lab @ BGU 2025 -Group GLAS"
author: "Gilad Sarusi, Lior Lipa, Adi Bitan and Sivan"
output: 
  pagedown::html_paged:
      self_contained: true
      paged-footnotes: true
      
format: 
  html:
    toc: true
    number-sections: true

editor: visual
---

# The Objective

Can chains of spontaneous thought reveal mental health? In this competition, you'll work with rich psychological data to predict depression levels from free association patterns and demographic variables.

Using a dataset of participants who completed a depression questionnaire (CES-D) and a 10-round free association task, our goal is to predict each participantâ€™s depression score using their unique trails of thought and background information.

For details about the challenge visit it's Kaggle page: [Depression Prediction Competitio](https://www.kaggle.com/competitions/depression-prediction/overview)

#Feature Engineering

{in this part we would add the acedemic background that helped us choose our feature and to desgin them in the way we will}

First and foremost, we will load th data.

```{r}
library(tidyverse)
library(tidymodels)
library(quanteda)

training_set <- read.csv('./data/ds_train_full.csv')

# DO NOT TOUCH THIS ONE! 
test_set <- read.csv('./data/ds_test_full.csv')

# The prompt given to the subjects in each block:
prompts_given <- data.frame(
  prompt = c(
  "log",
  "orb",
  "extension",
  "synthesizer",
  "treadmill",
  "valet",
  "toss",
  "testimony",
  "mow",
  "forge"),
  order = 1:10
)




# Since each subject goes through 10 blocks that has 10 steps it means that our training set consists of 51800/100 subjects (518)

# Since that this is Goal oriented challange where the justification of using one method instead of the other is relevant, we would use two method to split our traning set for validationL bootstrap (enable us to asses confidence) and Kfold CV (the standard and straightforward approach)

# NOTICE, the rows are not independent of one another- each subject has 100 rows so when splitting we need to ensure there's no data linkage. 

#Another consideration is class balance, we'll need to think whether or not we want strata sampling in bootstraps, meaning is there's any meaningful classes in training sets that might tilt the results towards one way due to unbalanced sampling in validation set vs the remaining set 

# ------ Pre-processing

#School need to be re arrange by order 
levels(factor(training_set$school))


# Income need to be re arrange by order 
levels(factor(training_set$Income))


hist(training_set$Age)
education_levels <- c(
  "Less than high school degree",
  "High school graduate (high school diploma or equivalent including GED)",
  "Some college but no degree",
  "Associate degree in college (2-year)",
  "Bachelor's degree in college (4-year)",
  "Master's degree",
  "Professional degree (JD, MD)",
  "Doctoral degree"
)
income_range_levels <- c(
  "Less than $10,000",
  "$10,000 to $19,999",
  "$20,000 to $29,999",
  "$30,000 to $39,999",
  "$40,000 to $49,999",
  "$50,000 to $59,999",
  "$60,000 to $69,999",
  "$70,000 to $79,999",
  "$80,000 to $89,999",
  "$90,000 to $99,999",
  "$100,000 to $149,999",
  "$150,000 or more"
)

#-------------------- Corpus, Embedding and Dictionaries ----------------

# Since   "docnames must be unique" we would create a word_id for every unique word

training_corpus <- training_set |>
  mutate(word_id =row_number()) |>  
  corpus(docid_field = "word_id",
         text_field="word")


# In order to use word_count methods we need that each participant's words (all 100 words) assign to him would be regarded as a text (let's say treatign his chains of words as one big 'tweet') then we would be able to count the the negative words in his chain

training_corpus_per_subject <- training_set |>
  group_by(ID) |>
  summarise(words_all = paste(word, collapse = " "), .groups = "drop") |>
  ungroup() |>  
  corpus(docid_field = "ID",
         text_field="words_all") 



# Tokenazation 

#due to the nature of the task I don't see aby reason to stems or lemmas, the same goes for not using N-Gram - the task's output is one-word each step


#Tokezating the words with the current data structure might be usful later 
training_tokens <- training_corpus |>
  tokens(  
    remove_punct = T,
    remove_symbols = T,
    remove_numbers = T) |>
  tokens_tolower()


neg_sentiment_words <-read.table("data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt",
                                 header = TRUE, sep = "\t") |>
  mutate(word = aback,
         sentiment = factor(anger),
         associate= X0) |>
  select(-aback,-anger,-X0) |>
  filter(sentiment== "negative" & associate == "1")


#------------------ Feature I - Negative Word Count -----------------  

training_corpus_per_subject_token_dfm <- training_corpus_per_subject  |>
  tokens(  
    remove_punct = T,
    remove_symbols = T,
    remove_numbers = T) |>
  tokens_tolower() |>
  dfm()


training_perS_neg_count <-training_corpus_per_subject_token_dfm |>
  dfm_lookup(
    dictionary(
      list(
        negative_count = neg_sentiment_words$word)
    )
  ) 

# let's add it to our training set

training_set <- training_set |>
  left_join(
    training_perS_neg_count |>
      convert("data.frame") |>
      rename(ID = doc_id) |>
      mutate(ID = as.numeric(ID)),
    by = "ID")

#Sanity Check:

table(training_set$negative_count)

hist(training_set$negative_count)\

#VERY NICE (BORAT VOICING)!!! 






















#--------------- Recipe ---------------------



rec <- recipe(matrixScore ~ ., data = training_set) |>
  step_rename(
    school_text = school,
    Income_text = Income
  ) |>
  step_mutate(
    
    
    #Setting the leveled vector  for school 
    school_text = factor(school_text,
                         levels = education_levels,
                         ordered = TRUE),
    # Creating an ordered school numeric column
    school_level = as.integer(school_text),
    
    #--------do the same for income: -----------
    Income_text = factor(Income_text,
                         levels=income_range_levels,
                         ordered = TRUE),
    Income_level = as.integer(Income_text),
    
    #------- Factoring the needed columns -------
    race_1 = factor(race_1),
    race_2 = factor(race_2),
    race_3 = factor(race_3),
    race_4 = factor(race_4),
    race_5 = factor(race_5),
    # since the 'Other' option in race_6 is manually typed I would regard it all a other even though we might grouped it in other manner
    race_6 = factor(race_6),
    
    sex = factor(sex)
    


  ) |>
  step_rm(
    Income_text,
    Income_level
  ) 

  




# See if the steps done correctly 
view(bake(prep(rec, training_set = training_data), NULL))




# ------ Validation ----------




```

{in this part we would add the acedemic background that helped us choose our feature and to desgin them in the way we will}

You can add options to executable code like this

```{r}
#| echo: false
2 * 2
```

The `echo: false` option disables the printing of code (only output is displayed).
