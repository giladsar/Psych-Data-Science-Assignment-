---
title: "Group Assignment: Reddit vs Twitter"
output:
  pagedown::html_paged:
    self_contained: true
    paged-footnotes: true
    
format:
  html:
    toc: true
    number-sections: true
    theme: default
---

# 1st Assignment - Sentiment Analysis

Read [Chapters 11-14](https://ds4psych.com/quantification).

Recall your research question and datasets from last assignment. Identify **two dictionary-based methods** that could be used to investigate your question. These could be either two different dictionaries (e.g. the NRC sadness dictionary and the NRC Affect Intensity sadness dictionary) or two different methodologies (e.g. raw word counts and lexical norms).

Look at your candidate dictionaries. What are advantages and disadvantages of each one? If you are proposing two methodologies, what advantages and disadvantages do they provide?

**Perform your analyses on both of your datasets** (i.e. four analyses total: two datasets × two methods). Do the results support your hypothesis? Are all four analyses similar, or are there differences? What aspects of the datasets or the methods might explain those differences?

Your group should turn in a report in Quarto html format. The report should include:

-   a summary of your datasets, dictionaries, methods, and hypotheses
-   all of your code (in code blocks), along with any output of the code
-   a discussion of advantages and disadvantages of each of the two methods or dictionaries used
-   a discussion of the results, identifying possible sources for any discrepencies between them
-   at least one graph that communicates your results

::: callout-tip
## Keep in Mind

Later assignments in the course will build on this one, so pick a research question that you are interested in investigating!
:::

## NRC Negative Emotions Lexicon

The lexicon is based on the NRC Word-Emotion Association Lexicon (EmoLex), developed by Mohammad & Turney (2010; 2013). EmoLex is a manually constructed emotion lexicon created through crowdsourcing and is based on Plutchik's model of eight basic emotions (1962, 1980, 1994). EmoLex contains thousands of words (unigrams), each categorized by human raters to one or more of the 8 basic emotions (joy, sadness, anger, fear, disgust, surprise, trust, and anticipation), as well as two general sentiment categories: positive and negative.

In the current study, we focused only on the negative emotions set, based on the theoretical assumption that these emotions may reflect experiences of emotional distress, particularly depression. Therefore, a reduced sub-lexicon was constructed, including only the words associated with negative sentiment. This category from EmoLex has been used in previous NLP studies to identify negative emotional content in social media texts, including in mental health contexts and depression detection (Kušen et al., 2017).

## Self-Focused Words Lexicon

This dictionary is based on the work of Gamoran, Kaplan, Simchon, and Gilead (2021), published as part of the CLPsych 2021 Shared Task. In that study, the researchers aimed to predict suicide attempts based on Twitter data, relying on prior psychological domain knowledge. They referred to a central finding from LIWC analyses on suicidal populations, which indicated an increase in words related to the self and a decrease in words related to others. Based on this theoretical insight, the researchers calculated the ratio between self-words (“I”) and group-words (“We”). In line with the findings of Eichstaedt et al. (2018), which demonstrated a link between increased use of self-focused language and depression, these linguistic features were assigned Bayesian priors informed by effect sizes reported in the existing literature.

In the current study, we used the list of self-words provided by Gamoran, Kaplan, Simchon, and Gilead (2021) and defined it as an independent dictionary for measuring self-focused words. Prior research has shown that language reflecting self-focus is a reliable predictor of depression. For example, Eichstaedt et al. (2018) found that Facebook users diagnosed with depression, or who later developed depression, were more likely to use self-referential words and phrases, such as first-person singular pronouns (“I,” “my”).

## Research Questions

1- Does depression among Twitter users predict the frequency of negative mood-related word use in their tweets compared to non-depressed users?

2- Does depression among Reddit users predict the frequency of negative mood-related word use in their posts compared to non-depressed users?

3- Does depression among Twitter users predict the frequency of SFA in their tweets compared to non-depressed users?

4- Does depression among Reddit users predict the frequency of SFA in their posts compared to non-depressed users?

### Twitter

```{r}
#| warning: false
library(tidyverse)
library(rvest)
library(glue)
library(quanteda)
library(SnowballC)
library(broom)
library(knitr)
library(quanteda.textstats)
library(quanteda.textplots)
library(sjPlot)
library(ggiraph, verbose = FALSE)
library(ggrepel)
library(ggwordcloud)
library(embedplyr)
library(text)
library(reticulate)

twitter_dep<- read_csv("data/Mental-Health-Twitter.csv")

twitter_dep_modify<- twitter_dep |>
  select(post_text, label) |>
  mutate(label= factor(label),
         post_text = post_text |> 
           str_replace_all( '@\\w+:|@\\w+', " ") |> 
           str_replace_all("RT ", " ") |> 
           str_replace_all("#", " "), 
         text_id = row_number()) 

 




twitter_corpus <- twitter_dep_modify |>
  corpus(docid_field = "text_id",
         text_field="post_text")




twitter_corpus_tokens <- tokens(twitter_corpus,
                         remove_punct = TRUE,
                         remove_symbols = T,
                         remove_url = T)
twitter_corpus_stems <- twitter_corpus_tokens |>
   tokens_wordstem()




# Note that we would not use Lemmas appraoch but stemed
# twitter_corpus_lemmas <- twitter_corpus_tokens |>
#     tokens_replace(pattern = lexicon::hash_lemmas$token, 
#                  replacement = lexicon::hash_lemmas$lemma) 



twitter_table_token_dfm <- dfm(twitter_corpus_stems)



# 1. Define the dictionary


#!!! switch to data.table for .txt file !!

text_sentiment_words <- read.table("data/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt", header = TRUE, sep = "\t") |>
  mutate(word = aback,
         sentiment = factor(anger),
         associate= X0) |>
  select(-aback,-anger,-X0) 

#creating the Negative Mood Dictionary 

neg_mood_df <- text_sentiment_words |>
  filter(sentiment== "negative" & associate == "1")

neg_mood_dict <- dictionary(
    list(
      negative = neg_mood_df[,1]
    )
  )

# Or alternatively, we could have just use: quanteda.sentiment::data_dictionary_NRC["negative"]



twitter_negative_dfm <- twitter_table_token_dfm |> 
  dfm_lookup(neg_mood_dict)



corpus_neg_mood_df <- twitter_negative_dfm |>
    convert("data.frame") |> # convert to dataframe
  right_join(
    twitter_corpus |> 
      convert("data.frame") # convert to dataframe
    )  |>
  mutate(total_words = ntoken(twitter_table_token_dfm))







neg_mod_glm_bino_ng <- MASS::glm.nb(negative ~ label + total_words + offset(log(total_words)),
                             data = corpus_neg_mood_df)


glm_X_neg_coef_tab <- tab_model(neg_mod_glm_bino_ng)
glm_X_neg_coef_tab
```

#### Results - depression, negative mood and twitter

As we can see all the coefficients are significant!

Twitter posts related to depression contain significantly more negative sentiment words, even when accounting for post length.

A negative association was found between post length and the proportion of negative sentiment words - the longer the post, the lower the proportion of negative words.

::: callout-tip
Note that the coefficient values shown are the exponentiated values of the GLM model’s original log-odds coefficients. This means they represent the odds ratios for the predictors, making it easier to interpret their effect on the outcome.
:::

### Reddit

```{r}
reddit_dep <- read.csv("data/reddit_depression_titles.csv") |>
  select(post_text = text) |>
  mutate(label = factor("depression"))

reddit_ctl <- read.csv("data/reddit_control3_titles.csv") |>
  select(post_text = text) |>
  mutate(label = factor("control"))

reddit_all_df <- bind_rows(reddit_dep, reddit_ctl) |>
  mutate(label = recode(factor(label),
                        "control" = "0",
                        "depression" = "1"),
         text_id = row_number())|>
  mutate(label = relevel(label, ref = "0"))

reddit_corpus <- corpus(reddit_all_df,
                        text_field = "post_text",
                        docid_field = "text_id")

reddit_tokens <- tokens(reddit_corpus,
                        remove_punct = TRUE,
                        remove_symbols = TRUE,
                        remove_url = TRUE) |>
  tokens_tolower()
reddit_corpus_stems <- tokens_wordstem(reddit_tokens)

   

# Same Here, we would work with the stemmed approach:

# reddit_corpus_lemmas <- tokens_replace(reddit_tokens,
#                                 pattern = lexicon::hash_lemmas$token,
#                                 replacement = lexicon::hash_lemmas$lemma,
#                                 valuetype = "fixed")
# 


reddit_table_token_dfm <- dfm(reddit_corpus_stems)

#crossing the Neg Mood dictionary with the reddit dfm:

reddit_negative_dfm <- reddit_table_token_dfm |> 
  dfm_lookup(neg_mood_dict)


reddit_corpus_neg_mood_df <- reddit_negative_dfm |>
    convert("data.frame") |> # convert to dataframe
  right_join(
    reddit_corpus |> 
      convert("data.frame") # convert to dataframe
    )  |>
  mutate(total_words = ntoken(reddit_table_token_dfm)) |>
  
  filter(total_words > 0)
  #getting rid of empty posts 
  
  

reddit_neg_mod_glm_bino_ng <- MASS::glm.nb(negative ~ label + total_words + offset(log(total_words)),
                             data = reddit_corpus_neg_mood_df)


tab_model(reddit_neg_mod_glm_bino_ng)


```

#### Results - Depression, negative mood and Reddit

Posts in Reddit about depression contain significantly more negative sentiment words compared to regular posts.

Post length (total_words) was found to be a significant predictor (p = 0.046) - there is a slight positive trend.

Therefore, the difference in emotional content stems from the post’s label and its length. by setting offset we controlled for post's length yet the label- Depressed or not- remained significant.

### X's Tweets - RQ3

```{r}
#fetching the self-focused words to form a dictunary: 
self_words_list <- read_csv("data/self_words_list.csv", col_names = TRUE)

#creating the dictionary
self_dict <- dictionary(
  list(
    self = self_words_list$word
    )
  )


#LAMMAS NOT STEMS! -----this time we would go with the lemmas approach since the validated dictionary is more suitable for this approach


twitter_corpus_lemmas <- twitter_corpus_tokens |>
     tokens_replace(pattern = lexicon::hash_lemmas$token, 
                 replacement = lexicon::hash_lemmas$lemma) 

twitter_dfm_lemmas <- dfm(twitter_corpus_lemmas)

self_twitter_lemmas_dfm <- dfm_lookup(twitter_dfm_lemmas, dictionary = self_dict)

twitter_corpus_self_df <- self_twitter_lemmas_dfm |>
    convert("data.frame") |> # convert to dataframe
  right_join(
    twitter_corpus |> 
      convert("data.frame") # convert to dataframe
    )  |>
  mutate(total_words = ntoken(twitter_table_token_dfm))


  

twitter_self_glm_bino_ng <- MASS::glm.nb(self ~ label + total_words + offset(log(total_words)),
                             data = twitter_corpus_self_df)

tab_model(twitter_self_glm_bino_ng)


```

#### Results - Twitter and self-focused talk

As we can see all the coefficients are significant!

Twitter posts related to depression contain significantly more self-focused words, even after adjusting for post length.

Post length also has a positive effect: the longer the post, the higher the number of self-focused words relative to its length.

### Reddit and self focused words - RQ4

```{r}


reddit_corpus_lemmas <- reddit_tokens |>
     tokens_replace(pattern = lexicon::hash_lemmas$token, 
                 replacement = lexicon::hash_lemmas$lemma) 

reddit_dfm_lemmas <- dfm(reddit_corpus_lemmas)

self_reddit_lemmas_dfm <- dfm_lookup(reddit_dfm_lemmas, dictionary = self_dict)

reddit_corpus_self_df <- self_reddit_lemmas_dfm |>
    convert("data.frame") |> # convert to dataframe
  right_join(
    reddit_corpus |> 
      convert("data.frame") # convert to dataframe
    )  |>
  mutate(total_words = ntoken(reddit_table_token_dfm)) |>
  
  #trimming out the empty posts 
  filter(total_words > 0 )


  

reddit_self_glm_bino_ng <- MASS::glm.nb(self ~ label + total_words + offset(log(total_words)),
                             data = reddit_corpus_self_df)

tab_model(reddit_self_glm_bino_ng)



```

#### Results- reddit and self focused words

As we can see all the coefficients are significant!

Posts about depression on Reddit contain significantly more self-focused words compared to regular posts.

However, the longer the post, the slightly (\<0.01) but significantly lower the proportion of self-focused words.

```{r}

#data vizualization : 


models_for_plot <- list(
  neg_mod_glm_bino_ng,
  twitter_self_glm_bino_ng,
  reddit_neg_mod_glm_bino_ng,
  reddit_self_glm_bino_ng
)
tidied_models <- map_dfr(models_for_plot, tidy, .id = "model") |> 
  select(model, term, estimate, std.error) |> 
  pivot_wider(names_from = term, values_from = estimate) |>
  mutate(Platform = ifelse(model == "1" |model ==  "2",
                           "Twitter",
                           "Reddit"),
         dict = ifelse(model == "1" |model == "3",
                       "Negative Mood",
                       " SFA")) |>
  filter(!is.na(label1))


  
  
ggplot(tidied_models, aes(x = Platform, y = label1, color = dict)) +
  geom_point(position = position_dodge(width = 0.5),
             size = 4) +
  geom_errorbar(aes(ymin = label1 -std.error,
                    ymax = label1 +std.error),
                width = 0.2,
                position = position_dodge(width = 0.5))  +
  labs(
    title = "The Effect of Depression on The Words Usages ",
    subtitle = "the Increased Usage of Negative Mood and SFA Words Between platforms Among Depressed Users Compared to Control",
    x = "Words Type",
    y = "Coeffiaciates (log odds)",
    color = "Social Platform"
  ) +
  theme_minimal(base_size = 10)
```

## Advantages and Disadvantages:

### NRC Negative Emotions Lexicon:

#### Advantages:

1- *Detection of explicit expressions of distress*: The lexicon is a reliable and effective tool for identifying emotional distress that is expressed explicitly. It includes words with a clearly negative emotional valence, allowing for the direct detection of negative emotions that are openly expressed in the text, without relying on indirect interpretation or latent indicators. This is particularly advantageous when analyzing naturalistic texts written by users themselves, without external guidance or mediation.

2- *High content and external validity*: EmoLex is based on systematic human judgments and relies on well-defined emotional categories derived from a recognized theoretical model (Plutchik, 1962, 1980, 1994), contributing to high content validity. In addition, its wide use in empirical studies in NLP, social media, and mental health supports its external validity as a tool for identifying negative emotions in text.

#### Disadvantages:

1- *Difficulty detecting implicit expressions of distress*: The lexicon is limited in its ability to detect negative emotions that are not explicitly expressed – that is, implicit. Social media texts may contain emotional distress conveyed indirectly, for example, through metaphors, irony, slang, cynicism, sarcasm, or vague phrasing. In such cases, a standard emotion lexicon may miss depressive discourse that is communicated in a subtle or implicit manner. This limitation may be particularly significant in Twitter texts, where discourse tends to be more indirect and implied due to the nature of the platform.

2- *Lack of specificity to depressive discourse*: The words in the lexicon are not unique to depressive contexts and may also appear in texts that do not reflect psychological distress. For example, words like "afraid" or "sad" may be used in everyday, humorous, or metaphorical contexts, without necessarily indicating a deep or pathological emotional state (e.g., from the dataset: "I got it... Geno is afraid to get hit...", where Geno is a nostalgic cartoon character). As a result, using such a lexicon may lead to false positives and reduce the precision in distinguishing between depressive discourse and general or temporary emotional expressions.

### Self-Words Lexicon:

#### Advantages:

1- *Detection of implicit expressions of distress*: Unlike negative emotion lexicons that focus on explicit and direct emotions, the self-words lexicon can capture instances of emotional distress even when they are not directly and openly articulated. Elevated self-focus, as reflected in frequent use of first-person singular pronouns, may indicate states associated with depression, even when the negative emotion is not explicitly conveyed. In this way, the lexicon contributes to detecting more subtle patterns of depressive discourse. This advantage may be particularly meaningful in texts from Twitter, where discourse tends to be more indirect and implied.

2- *Narrow scope and focused structure*: The lexicon is based on a limited number of words, all related to a specific linguistic category of first-person singular pronouns. This category is considered closed and clearly defined from a syntactic standpoint and includes the relevant words for this linguistic measure. This allows for full control over the measured content, accounting for language-based biases, and ensuring precise alignment with the theoretical framework of the research question, without relying on broad and potentially irrelevant data sources.

#### Disadvantages:

1- *Limited coverage (lack of content saturation)*: Due to its narrow and focused scope, the lexicon does not capture the full complexity of the depression phenomenon and thus has low content saturation. Additionally, the words it includes reflect various self-related language tendencies that do not necessarily indicate a depressive emotional state. For instance, using first-person singular pronouns may reflect self-focus that is not pathological.

2- *Prediction of other disorders*: Frequent use of first-person singular pronouns has also been observed in individuals experiencing other mental health conditions, such as social anxiety (Spurr & Stopa, 2002) or eating disorders (Zucker et al., 2015). Therefore, although it is a sensitive indicator of emotional distress, it does not uniquely distinguish depression from other psychological phenomena, which may reduce the interpretive precision of the findings and lead to false positives.

# 2nd Assignment - Vizualization and TF-IDF

Read [Chapters 15-16](https://ds4psych.com/dla).

Recall your research question, datasets, and analyses from the previous two assignments.

**Part 1:**

-   Generate a rotated F/F plot for each dataset, comparing the two groups in each.
-   Compute keyness statistics for tokens in both datasets, and plot the results with word clouds. Include only tokens with significant differences between the groups.

**Part 2:** Update your dictionary-based analyses using two different transformations from Chapter 16 (e.g. binary tokenization and TF-IDF). Do the results change? If so, what explains the difference? What do your chosen transformations contribute to the analysis?

Your group should turn in a report in Quarto html format. The report should include:

-   two rotated F/F plots (one for each dataset)
-   two word clouds (one for each dataset)
-   discussion of the new analysis and results
-   all code and output

## Part One

### FF Plot

#### Twitter

```{r}

twitter_corpus_tokens_non_stemmed_dfm <- twitter_corpus_tokens |>
  dfm()
twitter_frequency_table_by_label <- twitter_corpus_tokens_non_stemmed_dfm |> 
  textstat_frequency(groups = label)


twitter_frequency_ratio_label <- twitter_frequency_table_by_label |>
  pivot_wider(id_cols = "feature", 
              names_from = "group", 
              values_from = "frequency",
              names_prefix = "count_") |> 
  mutate(freq_dep= count_1/sum(count_1, na.rm = TRUE),
         freq_control = count_0/sum(count_0, na.rm = TRUE),
         dep_freq_ratio = freq_dep/freq_control) 

#Cool Now lets graph this thing up! 



ffplot_twitter <- twitter_frequency_ratio_label |> 
  mutate(
    # calculate total frequency
    common = (freq_dep + freq_control)/2,
    # remove single quotes (for html)
    feature = str_replace_all(feature, "'", "`")) |> 
  ggplot(aes(dep_freq_ratio, common, 
             label = feature,
             color = dep_freq_ratio,
             tooltip = feature, 
             data_id = feature
             )) +
    geom_point_interactive() +
    geom_text_repel_interactive(size = 2) +
    scale_y_continuous(
      trans = "log2", breaks = ~.x,
      minor_breaks = ~2^(seq(0, log2(.x[2]))),
      labels = c("Rare", "Common")
      ) +   
    scale_x_continuous(
      trans = "log10", limits = c(1/10,10),
      breaks = c(1/10, 1, 10),
      labels = c("10x More Common\nin Non-Depressive users",
                 "Equal Proportion",
                 "10x More Common\nin Depressive users")
      ) +
    scale_color_gradientn(
      colors = c("#023903", 
                 "#318232",
                 "#E2E2E2", 
                 "#9B59A7",
                 "#492050"), 
      trans = "log2", # log scale for ratios
      guide = "none"
      ) +
    labs(
      title = "Words in Depressed and Non-Depressed Users' Tweets ",
      x = "",
      y = "Total Frequency",
      color = ""
    ) +
    # fixed coordinates since x and y use the same units
    coord_fixed(ratio = 1/8) + 
    theme_minimal()


```

```{r}
#| fig-width: 15
#| fig-height: 10
#| fig-cap: "FFPlot for the Depression Twitter dataset"
#| label: fig-plot-2
girafe_options(
  girafe(ggobj = ffplot_twitter),
  opts_tooltip(css = "font-family:sans-serif;font-size:1em;color:Black;")
  )

```

By mere visual examination of the FF plot, it seems that words related to the political sphere are more frequently used by non-depressed users on Twitter (by 10 to 8 times more!). Such words include *"vote"* and *"election"*. Moreover, words that refer to a third person are also more commonly used among depressed users, such as *"him"*, *"his"*, and *"he"*.

On the other hand,unsuporisingly some of the words that are being used 10 times more by depressed users are *"pain"*, *cancer* and *disorder*. around 5 times more likely to use the word "*help*".

#### Reddit

```{r}

reddit_corpus_tokens_non_stemmed_dfm <- reddit_tokens |>
  dfm()


reddit_frequency_table_by_label <- reddit_corpus_tokens_non_stemmed_dfm |> 
  textstat_frequency(groups = label)


reddit_frequency_ratio_label <- reddit_frequency_table_by_label |>
  pivot_wider(id_cols = "feature", 
              names_from = "group", 
              values_from = "frequency",
              names_prefix = "count_") |> 
  mutate(freq_dep= count_1/sum(count_1, na.rm = TRUE),
         freq_control = count_0/sum(count_0, na.rm = TRUE),
         dep_freq_ratio = freq_dep/freq_control) 

#Cool Now lets graph this thing up! 


ffplot_reddit <- reddit_frequency_ratio_label |> 
  mutate(
    # calculate total frequency
    common = (freq_dep + freq_control)/2,
    # remove single quotes (for html)
    feature = str_replace_all(feature, "'", "`")) |> 
  ggplot(aes(dep_freq_ratio, common, 
             label = feature,
             color = dep_freq_ratio,
             tooltip = feature, 
             data_id = feature
             )) +
    geom_point_interactive() +
    geom_text_repel_interactive(size = 2) +
    scale_y_continuous(
      trans = "log2", breaks = ~.x,
      minor_breaks = ~2^(seq(0, log2(.x[2]))),
      labels = c("Rare", "Common")
      ) +   
    scale_x_continuous(
      trans = "log10", limits = c(1/10,10),
      breaks = c(1/10, 1, 10),
      labels = c("10x More Common\nin Non-Depressive users",
                 "Equal Proportion",
                 "10x More Common\nin Depressive users")
      ) +
    scale_color_gradientn(
      colors = c("#023903", 
                 "#318232",
                 "#E2E2E2", 
                 "#9B59A7",
                 "#492050"), 
      trans = "log2", # log scale for ratios
      guide = "none"
      ) +
    labs(
      title = "Words in Depressed and Non-Depressed Users' Posts on Reddit ",
      x = "",
      y = "Total Frequency",
      color = ""
    ) +
    # fixed coordinates since x and y use the same units
    coord_fixed(ratio = 1/8) + 
    theme_minimal()


```

```{r}
#| fig-width: 15
#| fig-height: 10
#| fig-cap: "FFPlot for the Depression Reddit dataset"
#| label: fig-plot-3
girafe_options(
  girafe(ggobj = ffplot_reddit),
  opts_tooltip(css = "font-family:sans-serif;font-size:1em;color:Black;")
  )

```

Align with the results of the GLM, depressed users in reddit uses more the word "I". Noteworthy, this time the words are more blunt than in twitter; depressed Reddit users uses up to 10 time words like: "*depressed*", "*severe*", "*suicide*" and "*ending*".

### Word Cloud and Keyness.

First I would Work My head around Keyness. Keyness overcomes the problems of using frequancy as a arealiable demension since it is sensitive to the number of words assigned to each group.

```{r}
  depresion_keyness_twitter <- twitter_corpus_tokens_non_stemmed_dfm |>
  dfm_trim(min_docfreq = 50) |>
  textstat_keyness(docvars(twitter_corpus_tokens_non_stemmed_dfm,
                           "label") == "1",
                   measure = "lr")

depresion_keyness_twitter |>
  textplot_keyness(labelsize = 2) +
  labs(title = "Words in Depression (target) and Non-Depression (reference) Stories")
```

Now that's more informative than FFplot!

Oh, but the task was to get the keyness statistic and plot a word cloud, so lets get it done:

first I would filter out the words that their frequancy is *NOT* significant different between the groups.

```{r}
#Since I already fetched the keyness of the entire tokens I would just filter out the unwanted words from the twitter corpus: 
depresion_keyness_twitter_sgnf_only <- depresion_keyness_twitter |>
  filter(p < 0.001 )


```

Overall, out of `r nrow(depresion_keyness_twitter)` words in the corpus (that appear at least 50 times), there’s a significant difference in the frequency of `r nrow(depresion_keyness_twitter_sgnf_only)` words when it comes to their usage in tweets.

I'll do the same for Reddit:

```{r}
  depresion_keyness_reddit <- reddit_corpus_tokens_non_stemmed_dfm |>
  dfm_trim(min_docfreq = 50) |>
  textstat_keyness(docvars(reddit_corpus_tokens_non_stemmed_dfm,
                           "label") == "1",
                   measure = "lr")


depresion_keyness_reddit_sgnf_only <- depresion_keyness_reddit |> 
  filter(p < 0.001)
```

Overall, out of `r nrow(depresion_keyness_reddit)` words in the Reddit corpus (that appear at least 50 times), there’s a significant difference in the frequency of `r nrow(depresion_keyness_reddit_sgnf_only)` words when it comes to their usage in Reddit's posts.

#### Twitter

```{r}
depresion_keyness_twitter_sgnf_only |> 
  arrange(desc(abs(G2))) |> 

  ggplot(aes(label = feature, 
             size = G2, 
             color = G2 > 0,
             angle_group = G2 > 0)) +
    geom_text_wordcloud_area(eccentricity = 1, show.legend = TRUE) + 
    scale_size_area(max_size = 30, guide = "none") +
    scale_color_discrete(
      name = "",
      breaks = c(FALSE, TRUE),
      labels = c("More in Non-Depressive Users", 
                 "More in Depressive Users")
      ) +
    labs(caption = "Only words with a significant difference between the groups (p < .001) are included.") +
    theme_void() # blank background
```

#### Reddit

```{r}
depresion_keyness_reddit_sgnf_only |> 
  arrange(desc(abs(G2))) |> 

  ggplot(aes(label = feature, 
             size = G2, 
             color = G2 > 0,
             angle_group = G2 > 0)) +
    geom_text_wordcloud_area(eccentricity = 1,
                             show.legend = TRUE,
                             try_fit = TRUE) + 
    scale_size_area(max_size = 25,
                    guide = "none") +
    scale_color_discrete(
      name = "",
      breaks = c(FALSE, TRUE),
      labels = c("More in Non-Depressive Users", 
                 "More in Depressive Users")
      ) +
    labs(caption = "Only words with a significant difference between the groups (p < .001) are included.") +
    theme_void() # blank background
```

## Part Two

### TF IDF

#### Twitter Negative

```{r}


twitter_negative_counter_dfm <- dfm_select(twitter_table_token_dfm,
                                           pattern = neg_mood_dict,
                                           case_insensitive = TRUE)
twitter_negative_tfidf_dfm <- dfm_tfidf(twitter_negative_counter_dfm)


twitter_negative_tfidf_df <- convert(twitter_negative_tfidf_dfm,
                                     to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(twitter_dep_modify, by = "text_id") |>
  mutate(negativity_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))


lm_twitter_neg_tfidf <- lm(negativity_score ~ label,
                            data = twitter_negative_tfidf_df)
summary(lm_twitter_neg_tfidf)

tab_model(lm_twitter_neg_tfidf)

```

#### Reddit Negative

```{r}


reddit_neg_counter_dfm <- dfm_select(reddit_table_token_dfm,
                                     pattern = neg_mood_dict,
                                     case_insensitive = TRUE)
reddit_neg_tfidf_dfm <- dfm_tfidf(reddit_neg_counter_dfm)

reddit_neg_tfidf_df <- convert(reddit_neg_tfidf_dfm,
                         to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(reddit_all_df, by = "text_id") |>
  mutate(negativity_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

lm_reddit_neg_tfidf <- glm(negativity_score ~ label,
                            data = reddit_neg_tfidf_df)
summary(lm_reddit_neg_tfidf)
tab_model(lm_reddit_neg_tfidf)

```

#### Twitter Self

```{r}

twitter_self_counter_dfm <- dfm_select(twitter_dfm_lemmas,
                               pattern = self_dict,
                               case_insensitive = T)
twitter_self_tfidf_dfm <- dfm_tfidf(twitter_self_counter_dfm)

twitter_self_tfidf_df <- convert(twitter_self_tfidf_dfm,
                           to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(twitter_dep_modify, by = "text_id") |>
  mutate(self_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

lm_twitter_self_tfidf <- glm(self_score ~ label,
                              data = twitter_self_tfidf_df)
summary(lm_twitter_self_tfidf)
tab_model(lm_twitter_self_tfidf)

```

#### Reddit Self

```{r}


reddit_self_counter_dfm <- dfm_select(reddit_dfm_lemmas,
                               pattern = self_dict,
                               case_insensitive = T)
reddit_self_tfidf_dfm <- dfm_tfidf(reddit_self_counter_dfm)

reddit_self_tfidf_df <- convert(reddit_self_tfidf_dfm, to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(reddit_all_df, by = "text_id") |>
  mutate(self_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

lm_reddit_self_tfidf <- glm(self_score ~ label,
                             data = reddit_self_tfidf_df)
summary(lm_reddit_self_tfidf)
tab_model(lm_reddit_self_tfidf)

```

We see a consistent pattern in our results this week: both the Reddit and Twitter models are statistically significant. This suggests that the TF-IDF method can be informative for our analysis, While TF-IDF is indeed sensitive to document length and word variety, our findings show that even in the shorter and more uniform Twitter posts, it captures meaningful differences between the groups. This strengthens our confidence in the robustness of the lexical signals we're detecting across platforms.

### Binary (Boolean) Tokenization

#### Twitter Negative

```{r}


twitter_negative_binary_dfm <- dfm_weight(twitter_negative_counter_dfm, scheme = "boolean")

twitter_negative_binary_df <- convert(twitter_negative_binary_dfm,
                              to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(twitter_dep_modify, by = "text_id") |>
  mutate(negativity_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

glm_twitter_negative_binary<- glm(label ~ negativity_score,
                                  data = twitter_negative_binary_df,
                                  family = binomial())
summary(glm_twitter_negative_binary)
tab_model(glm_twitter_negative_binary)
```

#### Reddit Negative

```{r}



reddit_neg_binary_dfm <- dfm_weight(reddit_neg_counter_dfm, scheme = "boolean")

reddit_neg_binary_df <- convert(reddit_neg_binary_dfm,
                                to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(reddit_all_df, by = "text_id") |>
  mutate(negativity_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

glm_reddit_neg_binary <- glm(label ~ negativity_score,
                           data = reddit_neg_binary_df,
                           family = binomial())
summary(glm_reddit_neg_binary)
tab_model(glm_reddit_neg_binary)
```

#### Twitter Self

```{r}


 
twitter_self_binary_dfm <- dfm_weight(twitter_self_counter_dfm,
                                      scheme = "boolean")

twitter_self_binary_df <- convert(twitter_self_binary_dfm,
                                  to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(twitter_dep_modify, by = "text_id") |>
  mutate(self_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

glm_twitter_self_binary<- glm(label ~ self_score,
                              data = twitter_self_binary_df,
                              family = binomial())
summary(glm_twitter_self_binary)
tab_model(glm_twitter_self_binary)
```

#### Reddit Self

```{r}

reddit_self_binary_dfm <- dfm_weight(reddit_self_counter_dfm,
                                     scheme = "boolean")

reddit_self_binary_df <- convert(reddit_self_binary_dfm,
                                 to = "data.frame") |>
  rename(text_id = doc_id) |>
  mutate(text_id = as.integer(text_id)) |>
  left_join(reddit_all_df, by = "text_id") |>
  mutate(self_score = rowSums(across(where(is.numeric), ~ replace_na(., 0))))

glm_reddit_self_binary <- glm(label ~ self_score,
                              data = reddit_self_binary_df,
                              family = binomial())
summary(glm_reddit_self_binary)
tab_model(glm_reddit_self_binary)

```

We see a clear pattern in our results: only the Reddit model using the negative emotion dictionary is statistically significant. All other models — including both Twitter models and both self-focus models — are not significant. This suggests that binary tokenization is limited in its ability to capture meaningful differences, especially in short or uniform texts like tweets. Reddit posts are longer and more expressive, so even a simple binary count of negative words can reveal group differences. In contrast, Twitter’s shorter and more repetitive format makes binary features less useful for distinguishing between depression and control groups.

# 3rd Assignment- Word Embedding

note to self while reading chapter 17: - Cosine similarity works best when your vector space is centered at zero (or close to it). In other words, it works best when zero represents a medium level of each variable. Not all vector spaces are zero-centered, so take a moment to consider the nature of your vector space before deciding which similarity or distance metric to use. - get your head around this: the correlation between two vectors is the same as the cosine similarity between them when the values of each vector are centered at zero.

Read [Chapters 17-20](https://ds4psych.com/vectorspace-intro).

Recall your research question and datasets from the previous two assignments.

1.  Investigate your research question using word embeddings. Are the results different from the ones you got with dictionary-based word counts? If so, how do you explain this difference?

2.  Investigate your research question using DDR. Are the results different from the ones you got with dictionary-based word counts? Are they different from the ones you got with word embeddings? If so, how do you explain these differences?

3.  Investigate your research question using CCR. Explain your choice of questionnaire for this task. Are the results different from the ones you got with the other methods? How do you explain the differences?

Your group should turn in a report in Quarto html format, including all code and output, explaining your analyses and results.

## Part I

First and foremost, for the reasons mentioned in the book the Word embedding approach we chose was *Word2vec*.

taking in consideration computation time and loading big data component like the model suggested in the book **GoogleNews.vectors.negative300**, we will store locally an .rds file for each corpus we're examing.

### Twitter Negative

To initiate, we need to work we with the complete Twitter's corpus tokenism not the stemmed ones:

```{r}

twitter_corpus_tokens_dfm <- dfm(twitter_corpus_tokens)

#getting all the features (words) in our twitter corpus:

twitter_features <- featnames(twitter_corpus_tokens_dfm)

#storing locally
word2vec_model_twitter <- load_embeddings(
  "GoogleNews.vectors.negative300",
  words = twitter_features,
  format = "rds",
  dir= "./rds/twitter_rds")


```

::: callout-tip
Why did we pass *"words = twitter_features"*? I assume it serves as test data, computing predicted values for each word in accordance with the trained model. However, the code in the example doesn't follow that logic
:::

Let's try to if a word like "Sadness" yields interesting results when considering the cos similarity of the twitter corpus

```{r}
#embedding the entire twitter corpus 
twitter_embbd_word2vec <- twitter_corpus_tokens_dfm |> 
textstat_embedding(word2vec_model_twitter)

```

```{r}




neg_word_embedding_twitter <- predict(word2vec_model_twitter,
                                          "sadness")
summary(neg_word_embedding_twitter)



  
#checking each word in the embedded corpus
twitter_embbd_neg_word2vec <- twitter_embbd_word2vec |> 
  get_sims(
    dim_1:dim_300, 
    list(neg = neg_word_embedding_twitter),
    method = "cosine_squished"
    )
  
  #adding the label (depresive or not)
  
twitter_embbd_neg_word2vec <- twitter_embbd_neg_word2vec |> 
  bind_cols(docvars(twitter_corpus_tokens))
  
twitter_neg_mod_word2vec <- betareg::betareg(
  neg ~ label, 
  twitter_embbd_neg_word2vec
  )
tab_model(twitter_neg_mod_word2vec)
summary(twitter_neg_mod_word2vec)


p_colmean_berareg <-summary(twitter_neg_mod_word2vec)$coefficients$mean["label1", "Pr(>|z|)"]


```

Currently the P-value of the bera regression of the in regarded to predicting the cos-similarity to "sadness" from the label is `r p_colmean_berareg`, which provides storng support for H1.

```{r}

#| echo: False
#| include: False
#| 
# Second Approach - Iteration 
# 
# p_values <- c()
# word_not_present_counter<- 0
# epsilon <- 1e-5 
# for (w in neg_words_chr){
#   
#   # embedding the word in questions
#   neg_word_embedding_twitter <- predict(word2vec_model_twitter, w) |> 
#     #ensuring that the values are bounded between 0 and 1
#     pmax(epsilon) |>
#     pmin(1 -epsilon)  
#   if (any(is.na(neg_word_embedding_twitter)) || length(neg_word_embedding_twitter) != 300) {
#   #skip if the word isn't in the embedded twitter model
#       word_not_present_counter <- word_not_present_counter + 1
#       next  
#     }
#   #checking each word in the embedded corpus
#   twitter_embbd_neg_word2vec <- twitter_embbd_word2vec |> 
#   get_sims(
#     dim_1:dim_300, 
#     list(neg = neg_word_embedding_twitter),
#     method = "cosine_squished"
#     )
#   #adding the label (depresive or not)
#   twitter_embbd_neg_word2vec <- twitter_embbd_neg_word2vec |> 
#   bind_cols(docvars(twitter_corpus_tokens))
#   
#   #Ensuring that labal has more then one Level
#   
#   n_levels <- nlevels(factor(twitter_embbd_neg_word2vec$label))
#   if (n_levels>1){
#       twitter_neg_mod_word2vec <- betareg::betareg(
#     neg ~ label, 
#     twitter_embbd_neg_word2vec
#   )}else {
#      next
#   }
#   p_values <- p_values |>
#     append(summary(twitter_neg_mod_word2vec)$coefficients$mean["label1",4])
# }
# 
# # computation time > 31 minutes ! 
# 
# avarage_pv_twitter_neg <- mean(p_values)
# 
# avarage_pv_twitter_neg

#[1] 0.05163386
```

::: callout-tip
After corresponding with Almog, the purpose of Part One has been clarified: to select a word that is closely related to our psychological construct - Depression.

In the case of *Twitter Negative*, we iterated over all words related to negative mood which is theoretically align with our construct, Depression. The resulting pv = 0.05163386 serves as a strong indicator that meets the goal of this part (examining the Twitter corpus).

From this point forward, the analysis will focus specifically on the word "Sadness", as it appears in the word embeddings derived from the Reddit corpus.
:::

### Reddit

```{r}

reddit_tokens_dfm <- dfm(reddit_tokens)

reddit_features <- featnames(reddit_tokens_dfm)

#creating model - giving the GoogleNew model contex

word2vec_model_reddit <- load_embeddings(
  "GoogleNews.vectors.negative300",
  words = reddit_features,
  format = "rds",
  dir= "./rds/reddit_rds")


# Embedding the  Reddit corpus
reddit_embbd_word2vec <- reddit_tokens_dfm |> 
textstat_embedding(word2vec_model_reddit)

```

Now, after setting up the vector space and embedding our corpus, let's Examine our question:

```{r}

# embedding the word 'sadness'
sadness_embedding_reddit <- predict(word2vec_model_reddit,
                                        "sadness") 
  
#checking each word in the embedded corpus
reddit_embbd_sadness_word2vec <- reddit_embbd_word2vec |>
get_sims(
  dim_1:dim_300,
  list(sadness = sadness_embedding_reddit),
  method = "cosine_squished"
)
#adding the label (depresive or not)
reddit_embbd_sadness_word2vec <- reddit_embbd_sadness_word2vec |>
  bind_cols(docvars(reddit_tokens))

#Ensuring that labal has more then one Level


reddit_mod_beta_word2vec <- betareg::betareg(
    sadness ~ label,
    reddit_embbd_sadness_word2vec
    )

tab_model(reddit_mod_beta_word2vec)


```

We used pretrained Word2Vec embeddings (GoogleNews, 300d) to examine the semantic similarity between words related to our construct, Depression, and the texts in our corpora.

Cosine similarity was computed between each document and a target vector— the embedded representation of the word "Sadness". Beta regression was then used to test for differences between depressed and non-depressed users.

Across platforms (Twitter and Reddit), we found significant group differences in the Reddit beta regression model (p \< .01). Depressed users consistently wrote posts that were more semantically similar to "Sadness" or to words associated with negative mood, suggesting heightened emotional negativity.

The Word Embeddings approach showed lower R² values for Twitter and higher for Reddit (0.027, 0.038) compared to the dictionary-based methods (0.111, 0.018), though it still achieved statistical significance.

These findings highlight the value of word embedding–based semantic measures in detecting psychological content—especially when contrasted with some of the earlier results, which were not always statistically significant.

## Part II- DDR

As notated earlier it seems that our second approach, using colMeans() to average the vectors of all the words in our dictionary. this time We would follow the functions and methods that is used in the book.

^*"DDR is ideal for studies of abstract constructs like emotions, that refer to the general gist of a text rather than particular words."*^

Although the recommendation is to use the 30 words that are most connected to the construct unfortunately would dictionary was build around binary decision making - whether the word is associated with the construct or not. so we will use it in its entirely but would weight it using their usage in our corpses, Twitter's tweets and Reddit posts (as fetched by Google News model- weighted using another corpus Google Trillion Word corpus.:

```{r}
#| results: "hide"
#| message: false
#| warning: false

#---------Embedded Dict --------------
#we already created neg_mood_dict earlier:
 
neg_mood_dict 

#Get the  dictionary words embeddings and wieghted by Google Trillion Word
neg_mood_twitter_ddr <- predict(word2vec_model_twitter,
                                neg_mood_dict$negative) |> 
    average_embedding(w= "trillion_word", na.rm = T)

##      Warning: 2076 items in `newdata` are not present in the embeddings object.


# Let's explore our negative mood dictionary using Reddit embedding 
neg_mood_reddit_ddr <- predict(word2vec_model_reddit,
                               neg_mood_dict$negative ) |> 
      average_embedding(w= "trillion_word", na.rm = T)
   

```

Let's see if this time it works:

Now, after we weighted the dictionary embedding (by each corpus) using the frequency in the Google Trillion Word corpus, we can use the embedded document (Reddit and Twitter corpus) dfm with our ~*docvar*~ properties to see if we get similar results

```{r}
#| results: "hide"

# we already have the dfm embedded:
reddit_embbd_word2vec
twitter_embbd_word2vec
```

Lets add the docvar: depression or control

**RUN THS ONLY ONCE! each time this code is executed a NEW columns of 'label' is created**

```{r}

reddit_embbd_word2vec <- reddit_embbd_word2vec |> 
  bind_cols(docvars(reddit_tokens))

twitter_embbd_word2vec<- twitter_embbd_word2vec |> 
  bind_cols(docvars(twitter_corpus_tokens))
```

Getting the cos_sims of the negative-mood-DDR in regard to each text (post or tweet).

```{r}
twitter_neg_mood_ddr_cossim <- twitter_embbd_word2vec |> 
  get_sims(
    dim_1:dim_300, 
    list(neg_mood = neg_mood_twitter_ddr), 
    method = "cosine_squished"
    )

reddit_neg_mood_ddr_cossim <- twitter_embbd_word2vec |> 
  get_sims(
    dim_1:dim_300, 
    list(neg_mood = neg_mood_reddit_ddr), 
    method = "cosine_squished"
    )
```

### Analysis

```{r}

# Reddit DDR Beta Regression Model: 

reddit_mod_beta_ddr_neg_mood <- betareg::betareg(
    neg_mood ~ label,
    reddit_neg_mood_ddr_cossim
    )

tab_model(reddit_mod_beta_ddr_neg_mood)

# Twitter DDR Beta Regression Model: 
twitter_mod_beta_ddr_neg_mood <- betareg::betareg(
    neg_mood ~ label,
    twitter_neg_mood_ddr_cossim
    )

tab_model(twitter_mod_beta_ddr_neg_mood)
```

The DDR method combined the Negative Mood dictionary with Word Embeddings, weighted by term frequency from the Google Trillion Word Corpus. The results were significant on both platforms: on Twitter, R² = 0.012 (p \< .001); on Reddit, R² = 0.013 (p \< .001). Compared to dictionary-based methods, DDR performed similarly to basic Word2Vec but with greater stability and control. It produced effect sizes comparable to Word2Vec while showing more consistent results across platforms. These differences may be explained by the fact that DDR integrates the strengths of both approaches. On one hand, it retains the psychological grounding and interpretability of dictionaries. On the other, it leverages the generalization power of embeddings and frequency-based weighting. This results in a method that is more stable than standalone Word2Vec, yet more flexible than raw word counts. The frequency weighting reduces the influence of rare and noisy terms, leading to more balanced results.

## Part III - CCR

Understanding contextualized word embedding:

^"Transformers start by converting all the words in a text into word embeddings, just like word2vec or GloVe. At the start, these word embeddings represent the average meaning of each word. The transformer then estimates how each word in the text might be relevant for better understanding the meaning of the other words. For example, if “circuit” appears right after “short”, the embedding of “short” should probably be tweaked. Once it has identified this connection, the transformer computes what “circuit” should add to a word that it is associated with, moving the “short” embedding closer to embeddings for electrical concepts."^

### Preparation

in order to use the 'text' package which is supported by Python I need to set up the environment.\

```{r}

# #installing the conda enviroment 
# # Create new conda environment
# conda_create("textr_env", packages = "python=3.10")
# 
# # Install dependencies into the conda environment - For debugging because errors appear
# conda_install("textr_env", c(
#   "pip", "hdbscan", "umap-learn", "bertopic", "scikit-learn", "pandas",
#   "nltk", "numpy", "transformers", "sentence-transformers", "evaluate", "datasets"
# ))
# 
# text::textrpp_install()
# 
# text::textrpp_initialize(save_profile = TRUE)


```

### Model - SBert

Creating the function that would contextualize embed our text:

```{r}

sbert_embeddings <- function(texts) {
  text::textEmbed(
    texts,
    model = "sentence-transformers/all-MiniLM-L12-v2", # model name
    layers = -2,  # second to last layer (default)
    tokens_select = "[CLS]", # use only [CLS] token
    dim_name = FALSE,
    keep_token_embeddings = FALSE
  )$texts[[1]]
} 


```

### PHQ-9

We decided to use the PHQ-9, as the alternative measures (such as the BDI and HAM-D) do not utilize a Likert-type scale to assess agreement with statements. Instead, each response level (e.g., 0 to 3 in the BDI) corresponds to a distinct descriptive statement. Therefore, we selected the PHQ-9, which aligns more closely with the example provided in the book.

::: callout-note
^Ask Almog about it ..^
:::

I would fetch the items using scraping, for the sport of it:

```{r}
#| results: "hide"
library(rvest)

html <- read_html("https://www.hiv.uw.edu/page/mental-health-screening/phq-9")


PHQ_items <- html |>  
  html_elements("h3") |> 
  html_text2() |> 
  {\(x) x[grepl("^\\d+\\. ", x)]}() |>
  gsub("^\\d+\\. ", "", x = _) 


# compute CCR by averaging item embeddings

depression_ccr <- PHQ_items |> 
  embed_docs(sbert_embeddings, output_embeddings = TRUE) |> 
  average_embedding()

#Embed the corpuses using contextual-embedding - sbert

# twitter_sbert <- twitter_dep_modify |>
#   embed_docs(
#     text_col = "post_text",
#     model = sbert_embeddings,
#     id_col = "text_id",
#     .keep_all = TRUE
#     )
# reddit_sbert <- reddit_all_df |>
#   embed_docs(
#     text_col = "post_text", 
#     model = sbert_embeddings, 
#     id_col = "text_id", 
#     .keep_all = TRUE
#     )
# 
# 
# twitter_depression_ccr <- twitter_sbert |> 
#   get_sims(
#     Dim1:Dim384,
#     list(depression = depression_ccr),
#     method = "cosine_squished"
#     )
# reddit_depression_ccr <- reddit_sbert |> 
#   get_sims(
#     Dim1:Dim384,
#     list(depression = depression_ccr),
#     method = "cosine_squished"
#     )
# 
# # beta regression
# Twitter_mod_ccr <- betareg::betareg(
#   depression ~ label,
#   twitter_depression_ccr
#   )
# 
#tab_model(Twitter_mod_ccr, file = "./fig_tab_models/twitter_mod_crr.html")



# 
# 
# Reddit_mod_ccr <- betareg::betareg(
#   depression ~ label,
#   reddit_depression_ccr
#   )
# 
#tab_model(Reddit_mod_ccr, file = "./fig_tab_models/Reddit_mod_crr.html")

# summary(Reddit_mod_ccr)
```

Due to the long computation time required to generate the tab_model() output for each corpus during rendering, the tables are presented as pre-rendered HTML figures:

::: table-container-reddit-ccr
***Reddit***

```{=html}
<!-- Reddit beta regression coeffiects - CCR  -->
<iframe src="fig_tab_models/Reddit_mod_crr.html" width="100%" height="400px" style="border:none;"></iframe>
```
:::

::: table-container-twitter-ccr
***Twitter***

```{=html}
<!-- Reddit beta regression coeffiects - CCR  -->
<iframe src="fig_tab_models/twitter_mod_crr.html" width="100%" height="400px" style="border:none;"></iframe>
```
:::

CCR yielded significant results across both platforms. The method outperformed previous approaches on several metrics, particularly in its ability to understand the context of words within a sentence, rather than just their presence. However this time testing the CCR in the Twitter corpus results in Depression explaining only 0.3% of the variance in cos_sim of CCR (R² = 0.012), whereas the same approach in the Reddit corpus yielded R² = 0.074.

These differences may be explained by CCR’s unique ability to interpret context. While Word2Vec and DDR rely on static word meanings, CCR uses Transformer-based models that recognize how a word's meaning shifts depending on context. This allows for more accurate identification of depressive expressions, even when indirect or context-dependent. For example, the word “tired” may have different meanings in “I’m tired of this situation” versus “I feel tired all the time.” CCR is better equipped to distinguish between these usages and detect the ones relevant to depression more precisely than the other methods.

# Bibliography:

Brockmeyer, T., Zimmermann, J., Kulessa, D., Hautzinger, M., Bents, H., Friederich, H. C., ... & Backenstrass, M. (2015). Me, myself, and I: self-referent word use as an indicator of self-focused attention in relation to depression and anxiety. *Frontiers in psychology*, *6*, 1564.‏

Eichstaedt, J. C., Smith, R. J., Merchant, R. M., Ungar, L. H., Crutchley, P., Preoţiuc-Pietro, D., ... & Schwartz, H. A. (2018). Facebook language predicts depression in medical records. *Proceedings of the National Academy of Sciences*, *115*(44), 11203-11208.‏

Gamoran, A., Kaplan, Y., Simchon, A., & Gilead, M. (2021, June). Using psychologically-informed priors for suicide prediction in the CLPsych 2021 shared task. In *Proceedings of the seventh workshop on computational linguistics and clinical psychology: Improving access* (pp. 103-109).‏

Kušen, E., Cascavilla, G., Figl, K., Conti, M., & Strembeck, M. (2017, August). Identifying emotions in social media: comparison of word-emotion lexicons. In *2017 5th international conference on future internet of things and cloud workshops (FiCloudW)* (pp. 132-137). IEEE.‏

Mohammad, S., & Turney, P. (2010). Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. *Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text*, 26–34.

Mohammad, S. M., & Turney, P. D. (2013). Crowdsourcing a word-emotion association lexicon. *Computational Intelligence*, *29*(3), 436–465.

Plutchik, R. (1980). A general psychoevolutionary theory of emotion. In *Theories of emotion* (pp. 3-33). Academic press.‏

Spurr, J. M., & Stopa, L. (2002). Self-focused attention in social phobia and social anxiety. *Clinical psychology review*, *22*(7), 947-975.‏

Zucker, N., Wagner, H. R., Merwin, R., Bulik, C. M., Moskovich, A., Keeling, L., & Hoyle, R. (2015). Self‐focused attention in anorexia nervosa. *International journal of eating disorders*, *48*(1), 9-14.‏
